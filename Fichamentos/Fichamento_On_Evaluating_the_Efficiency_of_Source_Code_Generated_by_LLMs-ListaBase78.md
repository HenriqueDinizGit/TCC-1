# On Evaluating the Efficiency of Source Code Generated by LLMs

Niu, Changan; Zhang, Ting; Li, Chuanyi; Luo, Bin; Ng, Vincent. "On Evaluating the Efficiency of Source Code Generated by LLMs." In IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (FORGE), April 14, 2024, Lisbon, Portugal, pp. 103-107, 2024. doi: 10.1145/3650105.3652295

## 1. Fichamento de Conteúdo

O artigo investiga a eficiência do código gerado por modelos de linguagem de grande porte (LLMs) – uma área pouco explorada em comparação à avaliação da correção do código. Os autores definem duas questões de pesquisa principais: (RQ1) medir a eficiência do código produzido por LLMs e (RQ2) investigar como diferentes prompts podem incentivar a geração de código mais eficiente. Para tal, o estudo utiliza três benchmarks: HumanEval, MBPP e um novo conjunto derivado do LeetCode, denominado LeetCodeEval, que abrange problemas de complexidade variada. São avaliados diversos modelos, como GPT-4, GPT-3.5, Code Llama, WizardCoder e DeepSeek Coder, comparando métricas como tempo de execução (runtime) e a taxa Pass@10. Os resultados indicam que a capacidade de gerar código correto não se correlaciona necessariamente com a eficiência do código, e que estratégias de prompting em cadeia (step-by-step) podem melhorar a performance em problemas complexos. Assim, o artigo propõe um caminho de pesquisa para aprimorar a eficiência dos LLMs na geração de código, oferecendo insights práticos para a seleção e o uso desses modelos.

## 2. Fichamento Bibliográfico

* **Runtime:** Tempo de execução do código gerado, usado como métrica principal para avaliar a eficiência dos LLMs.
* **Pass@10:** Métrica que indica a probabilidade de pelo menos uma das 10 amostras geradas passar em todos os testes, refletindo a performance dos modelos.
* **Normalized Runtime:** Técnica de normalização dos tempos de execução para possibilitar comparações justas entre os diferentes modelos de LLM.
* **Benchmarking:** Uso de benchmarks como HumanEval, MBPP e LeetCodeEval para medir e comparar a eficiência dos códigos gerados.

## 3. Fichamento de Citações

* "However, the efficiency of the generated code is overlooked."  
* "First, the ability to generate correct code is not positively correlated with the ability to generate efficient code."  
* "Step-by-step prompting could make LLM to generate more efficient code, especially on complex problems."

