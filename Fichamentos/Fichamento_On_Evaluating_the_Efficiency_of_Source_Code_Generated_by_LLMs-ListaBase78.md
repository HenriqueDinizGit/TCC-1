# On Evaluating the Efficiency of Source Code Generated by LLMs

Niu, Changan; Zhang, Ting; Li, Chuanyi; Luo, Bin; Ng, Vincent. "On Evaluating the Efficiency of Source Code Generated by LLMs." In IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (FORGE), April 14, 2024, Lisbon, Portugal, pp. 103-107, 2024. doi: 10.1145/3650105.3652295

## 1. Fichamento de Conteúdo

O artigo investiga a eficiência do código gerado por modelos de linguagem de grande porte (LLMs) – uma área pouco explorada em comparação à avaliação da correção do código. Os autores definem duas questões de pesquisa principais: (RQ1) medir a eficiência do código produzido por LLMs e (RQ2) investigar como diferentes prompts podem incentivar a geração de código mais eficiente. Para tal, o estudo utiliza três benchmarks: HumanEval, MBPP e um novo conjunto derivado do LeetCode, denominado LeetCodeEval, que abrange problemas de complexidade variada. São avaliados diversos modelos, como GPT-4, GPT-3.5, Code Llama, WizardCoder e DeepSeek Coder, comparando métricas como tempo de execução (runtime) e a taxa Pass@10. Os resultados indicam que a capacidade de gerar código correto não se correlaciona necessariamente com a eficiência do código, e que estratégias de prompting em cadeia (step-by-step) podem melhorar a performance em problemas complexos. Assim, o artigo propõe um caminho de pesquisa para aprimorar a eficiência dos LLMs na geração de código, oferecendo insights práticos para a seleção e o uso desses modelos.

## 2. Fichamento Bibliográfico

* **Eficiência do Código Gerado:** O estudo evidencia que a eficiência (medida pelo runtime) do código produzido pelos LLMs é uma dimensão distinta da sua correção funcional, demonstrando que modelos com maior acurácia nem sempre geram código mais rápido (aprox. p. 103-104).  
* **Benchmarks Utilizados:** São empregados os conjuntos HumanEval e MBPP para problemas de nível básico, além do LeetCodeEval para problemas mais complexos, permitindo uma comparação abrangente entre os modelos (aprox. p. 103-105).  
* **Prompting para Otimização:** Os autores exploram diferentes estratégias de prompting, inclusive prompts em cadeia, para incentivar os modelos a gerar soluções com menor tempo de execução, especialmente em problemas de maior complexidade (aprox. p. 105-106).  
* **Normalização do Runtime:** Para comparar de forma justa os diferentes códigos gerados, os runtimes são normalizados conforme uma metodologia inspirada nas práticas de plataformas como LeetCode e Codeforces (aprox. p. 104-105).

## 3. Fichamento de Citações

* "However, the efficiency of the generated code is overlooked."  
* "First, the ability to generate correct code is not positively correlated with the ability to generate efficient code."  
* "Step-by-step prompting could make LLM to generate more efficient code, especially on complex problems."

